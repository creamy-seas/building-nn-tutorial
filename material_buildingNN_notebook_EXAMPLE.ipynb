{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Keras\n",
    "########################################\n",
    "from keras.models import load_model                                                                     # load up models from file\n",
    "from keras import Model, Input                                                                          # major classes\n",
    "from keras.layers import Dense, Flatten, Dropout, LeakyReLU, BatchNormalization, Softmax, Embedding     # layers\n",
    "from keras.losses import categorical_crossentropy, mean_squared_error                                   # cost functions\n",
    "from keras.optimizers import Adam                                                                       # optimizer\n",
    "from keras.regularizers import l1, l2                                                                   # regularizers\n",
    "from keras.initializers import glorot_normal                                                            # for normalization\n",
    "\n",
    "########################################\n",
    "# Preparing data\n",
    "########################################\n",
    "from sklearn.model_selection import train_test_split # split into training and testing data\n",
    "\n",
    "# common packages\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Full workflow for the following NN\n",
    "<center>The origin of the data and what is means will be coverd in <a href=\"material_encodingData_notebook.ipynb\"><b>Encoding Notebook</b></a></center>\n",
    "\n",
    "<img src=\"images_inkscape/simple_model_v2.png\" style=\"width: 1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 1 ‚¶ø Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# 1 - load in the input data\n",
    "########################################\n",
    "with open(\"language_data.pkl\", 'rb') as fin:\n",
    "    data = pickle.load(fin)\n",
    "X_raw = data['X-input']\n",
    "Y_raw = data['Y-output']\n",
    "\n",
    "########################################\n",
    "# 2 - split into training and testing\n",
    "########################################\n",
    "(X_train, X_test,\n",
    " Y_train, Y_test) = train_test_split(X_raw,\n",
    "                                     Y_raw,\n",
    "                                     test_size = 0.02, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 2 üèó Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/CCCP/creamy_seas/sync_files/python_vi/pro_vi/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 15, 26)            0         \n",
      "_________________________________________________________________\n",
      "flatten_inputs (Flatten)     (None, 390)               0         \n",
      "_________________________________________________________________\n",
      "batchNorm_inputs (BatchNorma (None, 390)               1560      \n",
      "_________________________________________________________________\n",
      "1_dense (Dense)              (None, 1950)              762450    \n",
      "_________________________________________________________________\n",
      "1_dropout (Dropout)          (None, 1950)              0         \n",
      "_________________________________________________________________\n",
      "1_batch (BatchNormalization) (None, 1950)              7800      \n",
      "_________________________________________________________________\n",
      "1_activation (LeakyReLU)     (None, 1950)              0         \n",
      "_________________________________________________________________\n",
      "2_dense (Dense)              (None, 1950)              3804450   \n",
      "_________________________________________________________________\n",
      "2_batch (BatchNormalization) (None, 1950)              7800      \n",
      "_________________________________________________________________\n",
      "2_activation (LeakyReLU)     (None, 1950)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                19510     \n",
      "=================================================================\n",
      "Total params: 4,603,570\n",
      "Trainable params: 4,594,990\n",
      "Non-trainable params: 8,580\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lambd = 0.03\n",
    "dropout_rate = 0.3\n",
    "\n",
    "########################################\n",
    "# 3 - construct the layers\n",
    "########################################\n",
    "number_of_input_nodes = (X_train.shape[1] * X_train.shape[2])\n",
    "INPUTS = Input((X_train.shape[1], X_train.shape[2]))\n",
    "FLATTENED = Flatten(name=\"flatten_inputs\")(INPUTS)\n",
    "PROPAGATING_LAYER = BatchNormalization(name=\"batchNorm_inputs\")(FLATTENED)\n",
    "PROPAGATING_LAYER = Dense(5 * number_of_input_nodes, name=\"1_dense\",\n",
    "                          kernel_initializer=glorot_normal(seed=random.randint(1, 10000)),\n",
    "                          kernel_regularizer=l2(lambd))(PROPAGATING_LAYER)\n",
    "PROPAGATING_LAYER = Dropout(dropout_rate, name=\"1_dropout\")(PROPAGATING_LAYER)\n",
    "PROPAGATING_LAYER = BatchNormalization(name=\"1_batch\")(PROPAGATING_LAYER)\n",
    "PROPAGATING_LAYER = LeakyReLU(0.1, name='1_activation')(PROPAGATING_LAYER)\n",
    "PROPAGATING_LAYER = Dense(5 * number_of_input_nodes, name=\"2_dense\",\n",
    "                          kernel_initializer=glorot_normal(seed=random.randint(1, 10000)),\n",
    "                          kernel_regularizer=l2(lambd))(PROPAGATING_LAYER)\n",
    "PROPAGATING_LAYER = BatchNormalization(name=\"2_batch\")(PROPAGATING_LAYER)\n",
    "PROPAGATING_LAYER = LeakyReLU(0.1, name='2_activation')(PROPAGATING_LAYER)\n",
    "OUTPUT_LAYER = Dense(Y_train.shape[1])(PROPAGATING_LAYER)\n",
    "\n",
    "########################################\n",
    "# 4 - compile the model\n",
    "########################################\n",
    "learning_rate = 0.0001\n",
    "beta_1 = 0.85\n",
    "beta_2 = 0.99\n",
    "adam_tuned = Adam(lr=learning_rate, beta_1=beta_1, beta_2=beta_2)           # üêãüêãüêã custom now\n",
    "model = Model(inputs=INPUTS, outputs=OUTPUT_LAYER)\n",
    "model.compile(loss=mean_squared_error, optimizer=adam_tuned)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106996 samples, validate on 1081 samples\n",
      "Epoch 1/4\n",
      "106996/106996 [==============================] - 20s 188us/step - loss: 57.8910 - val_loss: 39.9987\n",
      "Epoch 2/4\n",
      " 14336/106996 [===>..........................] - ETA: 17s - loss: 38.4511"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "batch_size = 1024\n",
    "\n",
    "history_log = model.fit(X_train, Y_train,\n",
    "                        verbose=1, epochs=epochs,\n",
    "                        batch_size=batch_size, validation_split=0.01)\n",
    "\n",
    "# 7 - print result\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "ax.plot(history_log.history[\"loss\"], color='grey', linewidth=1, alpha=0.4)\n",
    "ax.plot(history_log.history[\"val_loss\"], color=\"orange\", linewidth=3)\n",
    "ax.set_title(\"Training history_log\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend([\"Train\", \"Valdiation\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 3 üîê save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def save_nn(model, metadata, data, save_name, save_folder=\"trained_nn\"):\n",
    "    \"\"\"\n",
    "    __ Parameters __\n",
    "    [ks.model] model:           to save\n",
    "    [dictionary] metadata:      dictionary with the metadata on the nn\n",
    "    [dictionary] data:          train and test data\n",
    "    [str] save_name:            name to save nn under\n",
    "    [str] save_folder:          folder where to save nn\n",
    "\n",
    "    __ Description __\n",
    "    Save the model and its metadata for future use\n",
    "    \"\"\"\n",
    "    # 1 - create folder to store the nn\n",
    "    if not os.path.exists(f\"./{save_folder}\"):\n",
    "        os.makedirs(save_folder)\n",
    "    # 2 - save the model\n",
    "    model.save(f\"{save_folder}/{save_name}.nn\")\n",
    "    # 3 - save metadata of the model\n",
    "    with open(f\"{save_folder}/{save_name}_metadata.pkl\", 'wb') as fout:\n",
    "        pickle.dump(metadata, fout)\n",
    "    # 4 - save training and testing data\n",
    "    with open(f\"{save_folder}/{save_name}_data.pkl\", 'wb') as fout:\n",
    "        pickle.dump(data, fout)\n",
    "        \n",
    "        \n",
    "def load_nn(save_name, save_folder=\"trained_nn\"):\n",
    "    \"\"\"\n",
    "    __ Parameters __\n",
    "    [str] save_name:            name the nn was saved under\n",
    "    [str] save_folder:          folder nn is in\n",
    "\n",
    "    __ Description __\n",
    "    Load the model and metadata\n",
    "\n",
    "    __ Returns __\n",
    "    [model] with the initialized weights from the file\n",
    "    [dictionary] metadata on the model\n",
    "    \"\"\"\n",
    "\n",
    "    # 1 - load the model\n",
    "    model = load_model(f\"./{save_folder}/{save_name}.nn\")\n",
    "    # 2 - load the metadata\n",
    "    with open(f'{save_folder}/{save_name}_metadata.pkl', 'rb') as fin:\n",
    "        metadata = pickle.load(fin)\n",
    "    # 3 - load training and testing data\n",
    "    with open(f'{save_folder}/{save_name}_data.pkl', 'rb') as fin:\n",
    "        data = pickle.load(fin)\n",
    "        \n",
    "    return model, metadata, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# 1 - generate a list of metadata for the model\n",
    "########################################\n",
    "metadata = {\"X-train_dim\": X_train.shape,\n",
    "            \"Y-train_dim\": Y_train.shape,\n",
    "            # anything else which would be required to recreate the model\n",
    "}\n",
    "train_test_data = {\"X-train\": X_train,\n",
    "                       \"X-test\": X_test,\n",
    "                       \"Y-train\": Y_train,\n",
    "                       \"Y-test\": Y_test}\n",
    "########################################\n",
    "# 2 - save\n",
    "########################################\n",
    "save_nn(model, metadata, train_test_data, \"language_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 4  Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¶ø  37.40% error - 825/2206 samples\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 1 ‚è∞  imagine some time has passed, or testing is perfromed on a different computer\n",
    "########################################\n",
    "model, metadata, data = load_nn(\"language_v1\")\n",
    "\n",
    "X_test = data['X-test']\n",
    "Y_test = data['Y-test']\n",
    "\n",
    "# 1 - run prediction\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "########################################\n",
    "# 2 -  find how many of the predictions were correct\n",
    "########################################\n",
    "prediction_mostLikely = np.argmax(prediction, axis=1)\n",
    "error_array = np.argmax(Y_test, axis=1) - prediction_mostLikely\n",
    "error_indicies = np.nonzero(error_array)[0]\n",
    "\n",
    "print(f\"‚¶ø  {len(error_indicies) / len(X_test) * 100:.2f}% error - {len(error_indicies)}/{len(X_test)} samples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "material_buildingNN_notebook_EXAMPLE.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
